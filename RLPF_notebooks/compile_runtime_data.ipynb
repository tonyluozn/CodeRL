{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script generate a single files that contains all gen_solutions result and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_solution_info(data_root_dir, problem_id):\n",
    "  json_file_path = data_root_dir+str(problem_id).zfill(4)+'/gen_solutions.json'\n",
    "  # check if the file exist\n",
    "  if not os.path.exists(json_file_path):\n",
    "    return [],[]\n",
    "  else:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "      data = json.load(file)\n",
    "      runtimes = []\n",
    "      results = []\n",
    "      for i in range(0, len(data)):\n",
    "        runtimes.append(data[i]['time'])\n",
    "        results.append(data[i]['result'])\n",
    "      return results, runtimes\n",
    "\n",
    "# save all data in json file\n",
    "json_data = {}\n",
    "for i in range(0, 5000):\n",
    "    print(i)\n",
    "    results, runtimes = get_gen_solution_info('data/APPS/train/', i)\n",
    "    json_data[i] = {}\n",
    "    json_data[i]['results'] = results\n",
    "    json_data[i]['runtimes'] = runtimes\n",
    "\n",
    "with open('models_eval/train_gen_results_runtimes.json', 'w') as file:\n",
    "    json.dump(json_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_runtime(data_root_dir, problem_id):\n",
    "  json_file_path = data_root_dir+str(problem_id).zfill(4)+'/gen_solutions.json'\n",
    "  # check if the file exist\n",
    "  if not os.path.exists(json_file_path):\n",
    "    return []\n",
    "  else:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "      data = json.load(file)\n",
    "      runtimes = []\n",
    "      for i in range(0, len(data)):\n",
    "        if data[i]['time'] != None:\n",
    "          runtimes.append(data[i]['time'])\n",
    "      return runtimes\n",
    "\n",
    "def get_baseline_runtime(data_root_dir, problem_id):\n",
    "    json_file_path = data_root_dir+str(problem_id).zfill(4)+'/baseline_solutions.json'\n",
    "    # check if the file exist\n",
    "    if not os.path.exists(json_file_path):\n",
    "        return []\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        if data[0]['time'] == None:\n",
    "            return []\n",
    "        return [data[0]['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = 'data/APPS/train/'\n",
    "problem_id_baseline_not_solved_gen_solved = []\n",
    "for i in range(0, 5000):\n",
    "    gen_runtime = get_gen_runtime(data_root_dir, i)\n",
    "    baseline_runtime = get_baseline_runtime(data_root_dir, i)\n",
    "    if len(gen_runtime) != 0 and len(baseline_runtime) == 0:\n",
    "        problem_id_baseline_not_solved_gen_solved.append(i)\n",
    "print(len(problem_id_baseline_not_solved_gen_solved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a,b,c,d,e,f = 0,0,0,0,0,0\n",
    "data_root_dir = 'data/APPS/train/'\n",
    "count_intro, count_inter, count_comp = 0,0,0\n",
    "for i in range(0, 5000):\n",
    "    print(f'Process problem {i}')\n",
    "    gen_runtime = get_gen_runtime(data_root_dir, i)\n",
    "    baseline_runtime = get_baseline_runtime(data_root_dir, i)\n",
    "    if len(gen_runtime) == 0 and len(baseline_runtime) == 0:\n",
    "      continue\n",
    "    if len(gen_runtime) !=0 and len(baseline_runtime) !=0:\n",
    "      gen_avg = np.mean(gen_runtime)\n",
    "      if gen_avg>baseline_runtime:\n",
    "        if 0<=i<=1999: c+=1\n",
    "        elif 2000<=i<=2360: e+=1\n",
    "        else: a+=1\n",
    "      else:\n",
    "        if 0<=i<=1999: d+=1\n",
    "        elif 2000<=i<=2360: f+=1\n",
    "        else: b+=1\n",
    "    if len(baseline_runtime)==0:\n",
    "        if 0<=i<=1999: count_inter+=1\n",
    "        elif 2000<=i<=2360: count_comp+=1\n",
    "        else: count_intro+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'count intro {count_intro}')\n",
    "print(f'count inter {count_inter}')\n",
    "print(f'count comp {count_comp}')\n",
    "print(f'a {a}')\n",
    "print(f'b {b}')\n",
    "print(f'c {c}')\n",
    "print(f'd {d}')\n",
    "print(f'e {e}')\n",
    "print(f'f {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot column plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `data` is your filled data structure\n",
    "data = {\n",
    "    'introductory': {\n",
    "        'gen_higher': a/2639,\n",
    "        'gen_lower': b/2639,\n",
    "        'no_bl':count_intro/2639\n",
    "    },\n",
    "    'interview': {\n",
    "        'gen_higher': c/2000,\n",
    "        'gen_lower': d/2000,\n",
    "        'no_bl': count_inter/2000\n",
    "    },\n",
    "    'competition': {\n",
    "        'gen_higher': e/361,\n",
    "        'gen_lower': f/361,\n",
    "        'no_bl': count_comp/361\n",
    "    }\n",
    "}\n",
    "\n",
    "# Categories\n",
    "difficulties = list(data.keys())\n",
    "categories = ['gen_higher', 'gen_lower','no_bl']\n",
    "\n",
    "# Data for bar heights\n",
    "gen_higher_values = [data[difficulty]['gen_higher'] for difficulty in difficulties]\n",
    "gen_lower_values = [data[difficulty]['gen_lower'] for difficulty in difficulties]\n",
    "no_bl_values = [data[difficulty]['no_bl'] for difficulty in difficulties]\n",
    "\n",
    "# Position of bars on x-axis\n",
    "ind = np.arange(len(difficulties))\n",
    "# Width of a bar\n",
    "width = 0.2  # Adjusted for three bars\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.bar(ind - width, gen_higher_values, width, label='Avg sampled runtime > baseline runtime')\n",
    "plt.bar(ind, no_bl_values, width, label='basline solution fails the tests')\n",
    "plt.bar(ind +width, gen_lower_values, width, label='Avg sampled runtime < baseline runtime')\n",
    "\n",
    "\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('% problems')\n",
    "plt.title('Comparison of Sampled and Baseline Solution Runtimes by Difficulty')\n",
    "\n",
    "# xticks()\n",
    "plt.xticks(ind, difficulties)  # Adjusted to align with the center of the groups\n",
    "\n",
    "# Adding legends\n",
    "plt.legend(loc='best')\n",
    "# Show the plot\n",
    "plt.savefig('models_eval/gen_baseline_runtimes_compare_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = 'data/APPS/train/'\n",
    "# for scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "gen_runtimes = []\n",
    "baseline_runtimes = []\n",
    "problem_ids = []\n",
    "problem_ids_bl = []\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    print(f'Process problem {i}')\n",
    "    gen_runtime = get_gen_runtime(data_root_dir, i)\n",
    "    baseline_runtime = get_baseline_runtime(data_root_dir, i)\n",
    "    if len(gen_runtime) == 0 or len(baseline_runtime) == 0:\n",
    "        continue\n",
    "    gen_runtimes.extend(gen_runtime)\n",
    "    baseline_runtimes.extend(baseline_runtime)\n",
    "    problem_ids.extend([i] * len(gen_runtime))\n",
    "    problem_ids_bl.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {}\n",
    "data['gen_runtimes'] = gen_runtimes\n",
    "data['baseline_runtimes'] = baseline_runtimes\n",
    "data['problem_ids'] = problem_ids\n",
    "data['problem_ids_bl'] = problem_ids_bl\n",
    "with open('models_eval/base_model_runtime_data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualatative analysis of the generated programs and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the results\n",
    "# load the above json file\n",
    "with open('models_eval/train_gen_results_runtimes.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "data['38']\n",
    "# 4174????? 4299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_count = []\n",
    "pass_3 = []\n",
    "for i in range(5000):\n",
    "  result = data[str(i)]['results']\n",
    "  passed = 0\n",
    "  for r in result:\n",
    "    if r == True:\n",
    "      passed +=1\n",
    "  pass_count.append(passed)\n",
    "  if passed ==3:\n",
    "    pass_3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create the histogram\n",
    "plt.hist(pass_count, bins=range(12), edgecolor='black', align='left')\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('Number of programs that passed all the unit test')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pass@10 analysis for APPS train data')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_problems)\n",
    "good_problems\n",
    "# Train:\n",
    "# interview: 0-1999\n",
    "# competition: 2000-2360\n",
    "# Intro: 2361-4999\n",
    "\n",
    "# intro: 2610 2649 2659 2742 2787 2805 3080\n",
    "# interview: 1439 1606\n",
    "# comp: 2002 2026 2055 2118 2191 2197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_variance_problems = []\n",
    "for i in good_problems:\n",
    "  runtime = data[str(i)]['runtimes']\n",
    "  if runtime and max(runtime)>200:\n",
    "    large_variance_problems.append(i)\n",
    "data['2787']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('models_eval/train_gen_results_runtimes.json', 'r') as file:\n",
    "    loaded_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_id = '2169'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = loaded_data[problem_id]['runtimes']\n",
    "file_path = f\"./outputs/train/codes/{problem_id}.json\"\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    # Extract the code entries\n",
    "    code_entries = data[problem_id][\"code\"]\n",
    "\n",
    "    # Print each code entry in a readable way\n",
    "    for idx, code in enumerate(code_entries, 1):\n",
    "        print(f\"Program {idx}:\\n\")\n",
    "        print(f'Runtime {runtimes[idx-1]}')\n",
    "        print(code)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
