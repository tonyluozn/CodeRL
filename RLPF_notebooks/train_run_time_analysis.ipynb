{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load runtime data for programs performance on APPS training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# load the json file\n",
    "with open('models_eval/base_model_runtime_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "gen_runtimes = data['gen_runtimes']\n",
    "baseline_runtimes = data['baseline_runtimes']\n",
    "problem_ids = data['problem_ids']\n",
    "problem_ids_bl = data['problem_ids_bl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce APPS Train Data Runtime Distribution Figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "problem_ids = np.array(problem_ids)\n",
    "gen_runtimes = np.array(gen_runtimes)\n",
    "problem_ids_bl = np.array(problem_ids_bl)\n",
    "baseline_runtimes = np.array(baseline_runtimes)\n",
    "\n",
    "# Separate data based on categories for gen solutions\n",
    "interview_mask_gen = (problem_ids <= 1999)\n",
    "competition_mask_gen = (problem_ids >= 2000) & (problem_ids <= 2360)\n",
    "intro_mask_gen = (problem_ids >= 2361) & (problem_ids <= 4999)\n",
    "\n",
    "# Separate data based on categories for baseline solutions\n",
    "interview_mask_bl = (problem_ids_bl <= 1999)\n",
    "competition_mask_bl = (problem_ids_bl >= 2000) & (problem_ids_bl <= 2360)\n",
    "intro_mask_bl = (problem_ids_bl >= 2361) & (problem_ids_bl <= 4999)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Interview scatter plot\n",
    "axs[1].scatter(problem_ids[interview_mask_gen], gen_runtimes[interview_mask_gen], label='gen solutions', alpha=0.5)\n",
    "axs[1].scatter(problem_ids_bl[interview_mask_bl], baseline_runtimes[interview_mask_bl], label='baseline solutions', alpha=0.5)\n",
    "axs[1].set_title('Interview Problems')\n",
    "axs[1].set_xlabel('Problem ID')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Competition scatter plot\n",
    "axs[2].scatter(problem_ids[competition_mask_gen], gen_runtimes[competition_mask_gen], label='gen solutions', alpha=0.6)\n",
    "axs[2].scatter(problem_ids_bl[competition_mask_bl], baseline_runtimes[competition_mask_bl], label='baseline solutions', alpha=0.6)\n",
    "axs[2].set_title('Competition Problems')\n",
    "axs[2].set_xlabel('Problem ID')\n",
    "axs[2].legend()\n",
    "axs[2].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Intro scatter plot\n",
    "axs[0].scatter(problem_ids[intro_mask_gen], gen_runtimes[intro_mask_gen], label='gen solutions', alpha=0.7)\n",
    "axs[0].scatter(problem_ids_bl[intro_mask_bl], baseline_runtimes[intro_mask_bl], label='baseline solutions', alpha=0.7)\n",
    "axs[0].set_title('Introductory Problems')\n",
    "axs[0].set_xlabel('Problem ID')\n",
    "axs[0].set_ylabel('Runtime (ms)')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models_eval/train_data_runtimes_by_difficulty.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np to calcaulte mean\n",
    "import numpy as np\n",
    "interview_runtime_Avg = np.mean(gen_runtimes[interview_mask_gen])\n",
    "competition_runtime_Avg = np.mean(gen_runtimes[competition_mask_gen])\n",
    "intro_runtime_Avg = np.mean(gen_runtimes[intro_mask_gen])\n",
    "print(\"Average runtime for interview problems: \", interview_runtime_Avg)\n",
    "print(\"Average runtime for competition problems: \", competition_runtime_Avg)\n",
    "print(\"Average runtime for intro problems: \", intro_runtime_Avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average runtime for interview problems:  25.750392004023134\n",
    "\n",
    "Average runtime for competition problems:  133.42541732283465\n",
    "\n",
    "Average runtime for intro problems:  4.329581621961442"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
